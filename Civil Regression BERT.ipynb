{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kent-mak/Anti-Hate-dashboard/blob/main/Civil%20Regression%20BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NFmd2EELpl2Z"
      },
      "outputs": [],
      "source": [
        "!pip install torch datasets transformers peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UEmeZnATalS"
      },
      "source": [
        "Define Model with LoRA for regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce9z1t-U9NGz",
        "outputId": "efd30412-c66b-484f-be70-229f07f7daec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.device(\"cuda\")\n",
        "else:\n",
        "    device_name = torch.device('cpu')\n",
        "print(\"Using {}.\".format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-iGTnxZr2rj"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, base_model, num_labels=7):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.model = base_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.regressor = nn.Linear(self.model.config.hidden_size, num_labels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[0][:, 0]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.regressor(pooled_output)\n",
        "        logits = self.sigmoid(logits)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.MSELoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI-Hupvew2uy"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import AutoModel\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "base_model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=4,                      # Rank of the low-rank matrices\n",
        "    lora_alpha=32,            # Scaling factor\n",
        "    lora_dropout=0.1,         # Dropout probability for LoRA layers\n",
        "    target_modules=[\"attention.q_lin\", \"attention.k_lin\", \"attention.v_lin\"]  # Modules to which LoRA will be applied\n",
        ")\n",
        "\n",
        "model_with_lora = get_peft_model(base_model, lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTwAj4ruxda-"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Integrate LoRA with the custom regression model\n",
        "model = RegressionModel(model_with_lora)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sze6dwC0ToE_"
      },
      "source": [
        "Work with the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYa_JfMW8iWO",
        "outputId": "31ea9242-2c22-4abe-e6fd-7256b300dfe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
            "    num_rows: 180487\n",
            "})\n",
            "Dataset({\n",
            "    features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
            "    num_rows: 9732\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "columns_to_check = ['toxicity', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
        "\n",
        "# Define a filter function\n",
        "def filter_all_zeros(example):\n",
        "    return any(example[col] != 0 for col in columns_to_check)\n",
        "\n",
        "percentage = '[:10%]'\n",
        "\n",
        "# dataset = load_dataset('google/civil_comments')\n",
        "training_set = load_dataset('google/civil_comments', split= f'train{percentage}')\n",
        "validation_set = load_dataset('google/civil_comments', split= f'validation{percentage}')\n",
        "test_set = load_dataset('google/civil_comments', split= f'test[:10%]')\n",
        "\n",
        "# filtered_training_set = training_set.filter(filter_all_zeros)\n",
        "# filtered_validation_set = validation_set.filter(filter_all_zeros)\n",
        "\n",
        "\n",
        "print(training_set)\n",
        "print(validation_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aevHIeh3SZIF"
      },
      "outputs": [],
      "source": [
        "# Define the preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples['text'], truncation=True, padding='max_length')\n",
        "    labels = torch.tensor(list(zip(\n",
        "        examples['toxicity'],\n",
        "        examples['severe_toxicity'],\n",
        "        examples['obscene'],\n",
        "        examples['threat'],\n",
        "        examples['insult'],\n",
        "        examples['identity_attack'],\n",
        "        examples['sexual_explicit']\n",
        "    )), dtype=torch.float32)\n",
        "\n",
        "    inputs[\"labels\"] = labels\n",
        "    return inputs\n",
        "\n",
        "# Apply the preprocessing function to each split\n",
        "# encoded_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "encoded_training_set = training_set.map(preprocess_function, batched=True, remove_columns=training_set.column_names)\n",
        "encoded_validation_set = validation_set.map(preprocess_function, batched=True, remove_columns=validation_set.column_names)\n",
        "encoded_test_set = test_set.map(preprocess_function, batched=True, remove_columns=test_set.column_names)\n",
        "print(encoded_training_set)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhPYHnqsTugO"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8HWTuDGSQy-"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "      # print(inputs)\n",
        "      labels = inputs.get(\"labels\").to(device_name)\n",
        "      model.to(device_name)\n",
        "      loss, logits = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=labels)\n",
        "      return (loss, logits) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kCbSl8L8ZLD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_training_set,\n",
        "    eval_dataset=encoded_validation_set,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "test_results = trainer.evaluate(encoded_test_set)\n",
        "print(f\"Test Results: {test_results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6BCk1ujlJcV"
      },
      "outputs": [],
      "source": [
        "baseRegressionModel = RegressionModel(base_model)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./base_results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "base_trainer = CustomTrainer(\n",
        "    model=baseRegressionModel,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "base_test_results = base_trainer.evaluate(encoded_test_set)\n",
        "print(f\"Test Results: {base_test_results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6yRuINSnakS"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"./model10%.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "loaded_model = RegressionModel(model_with_lora)\n",
        "loaded_model.to(device_name)\n",
        "loaded_model.load_state_dict(torch.load(\"./model5%.pt\"))\n",
        "loaded_model.eval()\n",
        "\n",
        "new_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "example_text = 'haha you guys are a bunch of losers.'\n",
        "input = new_tokenizer(example_text, return_tensors=\"pt\").to(device_name)\n",
        "\n",
        "with torch.no_grad():\n",
        "  logits = loaded_model(**input)\n",
        "\n",
        "print(logits)\n",
        "# print(loss)"
      ],
      "metadata": {
        "id": "BWShXgMnG3yN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "mount_file_id": "16kFy8uzvFB5RsUB9I46KolTgbv1-Xcbx",
      "authorship_tag": "ABX9TyNsc0L4wtAnDdFgZEJhEfZy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}